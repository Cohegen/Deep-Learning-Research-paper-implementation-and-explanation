# FlashAttention layer (custom CUDA/PyTorch)
